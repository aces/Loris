{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "LORIS-API_Part3-Create_image_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "vhBrpw8LogOy",
        "44HUGcwa7Zxu",
        "I6NO6Mvdrt8e",
        "MCjiEneS7Zx6",
        "UKLJB4k77Zx8",
        "H-u_y4QJ7ZyT",
        "-jSSONcA7ZyY",
        "iBC0xb937ZyG",
        "Q7QltcmV7ZyI",
        "EFcnVs-e7ZyN"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53B8lztu7ZxQ"
      },
      "source": [
        "# Image analysis with fMRI 3D images imported with LORIS API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byKR2jTxmurK"
      },
      "source": [
        "This is a tutorial to show how to use Loris' API to download MRI images. It also contains a few examples of how the data can be used to run basic data analysis. \n",
        "\n",
        "This tutorial is also available as a Google colab notebook so you can run it directly from your browser. To access it, click on the button below: <a href=\"https://colab.research.google.com/github/spell00/Loris/blob/2020-08-06-JupyterCreateImageDataset/docs/notebooks/LORIS-API_Part3-Create_image_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1umWrZ87ZxR"
      },
      "source": [
        "# Uncomment and run to install the packages required to run the notebook\n",
        "# !pip3 install tqdm\n",
        "# !pip3 install numpy\n",
        "# !pip3 install nibabel\n",
        "# !pip3 install sklearn\n",
        "# !pip3 install matplotlib\n",
        "# !pip3 install nilearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhBrpw8LogOy"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zjglBuY7ZxY"
      },
      "source": [
        "import getpass        # For input prompt not to show what is entered\n",
        "import json           # Provide convenient functions to handle json objects \n",
        "import re             # For regular expression\n",
        "import requests       # To handle http requests\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import warnings\n",
        "from tqdm import tqdm_notebook as tqdm  # To make a nice progress bar\n",
        "import os\n",
        "import itertools\n",
        "\n",
        "warnings.simplefilter('ignore') # Because I am using unverified ssl certificates \n",
        "\n",
        "baseurl = 'https://demo.loris.ca/api/v0.0.3' # Pick yours\n",
        "\n",
        "def prettyPrint(string):\n",
        "    print(json.dumps(string, indent=2, sort_keys=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W8AqTUg7Zxh"
      },
      "source": [
        "## Login\n",
        "\n",
        "This is a POST request to the /login endpoint that requires 2 parameters: username and password\n",
        "The expected response is a json string that contains a token property.\n",
        "\n",
        "For the complete API documentation, go here: https://github.com/aces/Loris/blob/main/modules/api/docs/LorisRESTAPI.md"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFh2IHin7Zxi"
      },
      "source": [
        "payload = {\n",
        "    'username': input('username: '), \n",
        "    'password': getpass.getpass('password: ')\n",
        "}\n",
        "\n",
        "response = requests.post(\n",
        "    url = baseurl + '/login',\n",
        "    json = payload,\n",
        "    verify = False\n",
        ")\n",
        "\n",
        "text = response.content.decode('ascii')\n",
        "data = json.loads(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9sNzFH3onfa"
      },
      "source": [
        "*Store the token in a variable for later*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ6ZMaYyo3jE"
      },
      "source": [
        "token = data['token']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSuT1EeV7Zxo"
      },
      "source": [
        "## Getting the data\n",
        "\n",
        "The data on https://demo.loris.ca are only for development purposes. Nevertheless, with this in mind, we will use it for demonstration purposes only. In this tutorial, we will download all the T1 and T2 raw images from every project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44HUGcwa7Zxu"
      },
      "source": [
        "#### Then, we get the information necessary to retrieve all images from all the projects and store them in a dictionnary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd4_1kbK7Zxu"
      },
      "source": [
        "# Find the project name and site name.\n",
        "projects = json.loads(requests.get(\n",
        "    url = baseurl + '/projects',\n",
        "    verify = False,\n",
        "    headers = {'Authorization': 'Bearer %s' % token}\n",
        ").content.decode('ascii'))\n",
        "\n",
        "projectnames = list(projects['Projects'])\n",
        "\n",
        "# Create the dictionnary to store the images informations to download them.\n",
        "imagesMeta = dict.fromkeys(projectnames)\n",
        "\n",
        "for project in projectnames:\n",
        "    imagesMeta[project] = json.loads(requests.get(\n",
        "        url = baseurl + '/projects/' + project + '/images',\n",
        "        verify = False,\n",
        "        headers = {'Authorization': 'Bearer %s' % token}\n",
        "            ).content.decode('ascii'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6NO6Mvdrt8e"
      },
      "source": [
        "### Download the data\n",
        "\n",
        "We use the urls we got in the previous step to download all T1 and T2 fMRI images. The images are stored in a dictionnary in their according groups (T1 or T2) for later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAAnRT1wBDmG"
      },
      "source": [
        "# The dictionary to store the images\n",
        "images_dict = {\n",
        "    \"raw\": {\n",
        "        't1': [],\n",
        "        't2': []\n",
        "    },\n",
        "    \"32x32x32\": {\n",
        "        't1': [],\n",
        "        't2': []\n",
        "    },\n",
        "    \"128x128x128\": {\n",
        "        't1': [],\n",
        "        't2': []\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPtiKIh37Zx1"
      },
      "source": [
        "# Progress bar for downloads\n",
        "pbar = tqdm(total=sum([len([meta for meta in imagesMeta[p]['Images'] if meta['ScanType'] in ['t1', 't2']]) for p in projectnames]))\n",
        "\n",
        "for project in projectnames:\n",
        "    for i, meta in enumerate(imagesMeta[project]['Images']):\n",
        "        if(meta['ScanType'] not in ['t1', 't2']):\n",
        "            continue\n",
        "        r = requests.get(baseurl + meta['Link'], \n",
        "                     headers = {'Authorization': 'Bearer %s' % token})\n",
        "        page = r.content\n",
        "        filename = meta['Link'].split('/')[-1]\n",
        "        t = meta['ScanType']\n",
        "\n",
        "        # The images need to be saved first.\n",
        "        # Only t1 and t2 images are kept. \n",
        "        if (t in ['t1', 't2']):\n",
        "            file_ = open(filename, 'wb')\n",
        "        else:\n",
        "            continue\n",
        "        file_.write(page)\n",
        "        file_.close()\n",
        "        img = nib.load(filename)\n",
        "\n",
        "        # The images are not necessary for the rest of this tutorial.\n",
        "        os.remove(filename) \n",
        "\n",
        "        img = img.get_fdata()\n",
        "\n",
        "        # The images are save in the dictionary\n",
        "        if(meta['ScanType'] == 't1'):\n",
        "            images_dict[\"raw\"][\"t1\"] += [img]\n",
        "        if(meta['ScanType'] == 't2'):\n",
        "            images_dict[\"raw\"][\"t2\"] += [img]\n",
        "        pbar.update(1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCjiEneS7Zx6"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o48-OGiI7Zx7"
      },
      "source": [
        "In this section, we'll explore a few preprocessing methods that might make the models learned perform better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKLJB4k77Zx8"
      },
      "source": [
        "### Resize images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgIzoalR7Zx8"
      },
      "source": [
        "In this tutorial, T1 and T2 images are compared. They are of similar sizes (160x256x224 and 160x256x256 for T1 and T2, respectively), but they need to be exactly the same size for any subsequent analysis.\n",
        "\n",
        "In machine learning, it is common practice to reduce large images before training a model. Large images have the advantage of containing more information, but it comes with a tradeoff known as the Curse of dimensionality. Having a high dimensionality can make it much easier to have good performances on the training set, but the models trained overfit more easily to the training data and perform poorly on the validation and test data.\n",
        "\n",
        "Of course, reducing images too much will also harm the performance of the model trained. There is no rule of thumb or algorithm to get the optimal size of images to be used in a specific task, so it might be a good idea to try a few different reductions.\n",
        "\n",
        "This tutorial will explore 2 dimensions. Both will cubes (all sides have the same length): 128x128x128 and 32x32x32. The later dimensions might be a huge reduction, but the 3D images still have 32,768 dimensions (each voxel being a dimension), which is still huge, but much more manageable than the larger reduction, which has 2,097,152 dimensions. In order to decide which reduction to use, we will observe the data using a Principal Component Analysis (PCA). It will give an idea of whether the data has lost too much information to use it in a classification task. \n",
        "\n",
        "Ultimately, it might be necessary to use both strategies to test if one is better than the other. In case both strategies appear to be equal, Ockham's razor principle suggest the images with fewer voxels should be used. In this case, the notion of equality is somewhat arbitrary and might depend on the task to be accomplished."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4c1snQS7Zx9"
      },
      "source": [
        "def resize_image(image, new_size=(160, 160, 160)):\n",
        "    \"\"\"\n",
        "    Function to resize an image.\n",
        "    Args:\n",
        "        image (Numpy array of shape (Length, Width, Depth)): image to transform\n",
        "        new_size (3-Tuple) : The new image length, width and Depth\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    initial_size_x = image.shape[0]\n",
        "    initial_size_y = image.shape[1]\n",
        "    initial_size_z = image.shape[2]\n",
        "\n",
        "    new_size_x = new_size[0]\n",
        "    new_size_y = new_size[1]\n",
        "    new_size_z = new_size[2]\n",
        "\n",
        "    delta_x = initial_size_x / new_size_x\n",
        "    delta_y = initial_size_y / new_size_y\n",
        "    delta_z = initial_size_z / new_size_z\n",
        "\n",
        "    new_image = np.zeros((new_size_x, new_size_y, new_size_z))\n",
        "\n",
        "    for x, y, z in itertools.product(range(new_size_x),\n",
        "                                     range(new_size_y),\n",
        "                                     range(new_size_z)):\n",
        "        new_image[x][y][z] = image[int(x * delta_x)][int(y * delta_y)][int(z * delta_z)]\n",
        "\n",
        "    return new_image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZp1D_RM7ZyB"
      },
      "source": [
        "\n",
        "We need to create new directeories to save the resized T1 and T2 images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-u_y4QJ7ZyT"
      },
      "source": [
        "#### Resize and normalize all T1 images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0-zfwth7ZyU"
      },
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "pbar = tqdm(total=len(images_dict['raw']['t1']))\n",
        "\n",
        "for t1 in images_dict['raw'][\"t1\"]:\n",
        "    t1_32 = resize_image(t1, (32, 32, 32))\n",
        "    t1_32 = Normalizer().fit_transform(t1_32.reshape([1, -1]))\n",
        "    t1_32 = t1_32.reshape([-1, 32, 32, 32])\n",
        "    images_dict['32x32x32']['t1'] += [t1_32]\n",
        "\n",
        "    t1_128 = resize_image(t1, (128, 128, 128))\n",
        "    t1_128 = Normalizer().fit_transform(t1_128.reshape([1, -1]))\n",
        "    t1_128 = t1_128.reshape([-1, 128, 128, 128])\n",
        "    images_dict['128x128x128']['t1'] += [t1_128]\n",
        "\n",
        "    pbar.update(1)\n",
        "\n",
        "    \"\"\"\n",
        "    We don't need to save the images for this tutorial, but the package nibabel\n",
        "    can be used to save the images to disk like this:\n",
        "\n",
        "    img = nib.Nifti1Image(image_to_save, np.eye(4))\n",
        "    img.to_filename(\"/path/to/new_file_name.nii\")\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "# Make numpy arrays from the lists of numpy arrays\n",
        "images_dict['32x32x32']['t1'] = np.stack(images_dict['32x32x32']['t1'])\n",
        "images_dict['128x128x128']['t1'] = np.stack(images_dict['128x128x128']['t1'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jSSONcA7ZyY"
      },
      "source": [
        "#### Resize and normalize T2 images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCM3wGx95Rk3"
      },
      "source": [
        "pbar = tqdm(total=len(images_dict['raw']['t2']))\n",
        "\n",
        "for t2 in images_dict['raw'][\"t2\"]:\n",
        "    t2_32 = resize_image(t2, (32, 32, 32))\n",
        "    t2_32 = Normalizer().fit_transform(t2_32.reshape([1, -1]))\n",
        "    t2_32 = t2_32.reshape([-1, 32, 32, 32])\n",
        "    images_dict['32x32x32']['t2'] += [t2_32]\n",
        "\n",
        "    t2_128 = resize_image(t2, (128, 128, 128))\n",
        "    t2_128 = Normalizer().fit_transform(t2_128.reshape([1, -1]))\n",
        "    t2_128 = t2_128.reshape([-1, 128, 128, 128])\n",
        "    images_dict['128x128x128']['t2'] += [t2_128]\n",
        "\n",
        "    pbar.update(1)\n",
        "\n",
        "# Make numpy arrays from the lists of numpy arrays\n",
        "images_dict['32x32x32']['t2'] = np.stack(images_dict['32x32x32']['t2'])\n",
        "images_dict['128x128x128']['t2'] = np.stack(images_dict['128x128x128']['t2'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBC0xb937ZyG"
      },
      "source": [
        "### Visualisation with nilearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AnYizA27ZyH"
      },
      "source": [
        "Visualisation of the raw images and the 2 reductions for T1 and T2 images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7QltcmV7ZyI"
      },
      "source": [
        "#### T1 images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-x872AK7ZyJ"
      },
      "source": [
        "# This package is used to plot a section of the 3D images\n",
        "import nilearn.plotting as nlplt  \n",
        "\n",
        "print(\"Original (160x256x224)\")\n",
        "t1_fullimage = nib.Nifti1Image(images_dict['raw']['t1'][0], np.eye(4))\n",
        "nlplt.plot_anat(t1_fullimage, (80, 128, 112))\n",
        "nlplt.show()\n",
        "\n",
        "print(\"128x128x128\")\n",
        "img_t1_128 = nib.Nifti1Image(resize_image(images_dict['raw']['t1'][0], (128, 128, 128)), np.eye(4))\n",
        "nlplt.plot_anat(img_t1_128, (64, 64, 64))\n",
        "nlplt.show()\n",
        "\n",
        "print(\"32x32x32\")\n",
        "img_t1_32 = nib.Nifti1Image(resize_image(images_dict['raw']['t1'][0], (32, 32, 32)), np.eye(4))\n",
        "nlplt.plot_anat(img_t1_32, (16, 16, 16))\n",
        "nlplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFcnVs-e7ZyN"
      },
      "source": [
        "#### T2 images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs15aUQ87ZyO"
      },
      "source": [
        "print(\"Original (160x256x256)\")\n",
        "t2_fullimage = nib.Nifti1Image(images_dict['raw']['t2'][0], np.eye(4))\n",
        "nlplt.plot_anat(t2_fullimage, (80, 128, 112))\n",
        "nlplt.show()\n",
        "\n",
        "print(\"128x128x128\")\n",
        "img_t2_128 = nib.Nifti1Image(resize_image(images_dict['raw']['t2'][0], (128, 128, 128)), np.eye(4))\n",
        "nlplt.plot_anat(img_t2_128, (64, 64, 64))\n",
        "nlplt.show()\n",
        "\n",
        "print(\"32x32x32\")\n",
        "img_t2_32 = nib.Nifti1Image(resize_image(images_dict['raw']['t2'][0], (32, 32, 32)), np.eye(4))\n",
        "nlplt.plot_anat(img_t2_32, (16, 16, 16))\n",
        "nlplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTlkJeMQ7Zyd"
      },
      "source": [
        "## Unsupervised learning: Principal Component Analysis\n",
        "\n",
        "Principal Component Analysis (PCA) is a popular method used for dimensioanlity reduction, which is a good first step to vizualise the data to analyse and can give insight for the subsequent steps of the analysis. Dimensionality reduction can also be used to transform the data before using it to train a ML model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m2kChEp7Zye"
      },
      "source": [
        "# sklearn needs the data to be flattened\n",
        "images_dict['32x32x32']['t1'] = images_dict['32x32x32']['t1'].reshape(\n",
        "    [images_dict['32x32x32']['t1'].shape[0], -1]\n",
        ")\n",
        "images_dict['128x128x128']['t1'] = images_dict['128x128x128']['t1'].reshape(\n",
        "    [images_dict['128x128x128']['t1'].shape[0], -1]\n",
        ")\n",
        "\n",
        "images_dict['32x32x32']['t2'] = images_dict['32x32x32']['t2'].reshape(\n",
        "    [images_dict['32x32x32']['t2'].shape[0], -1]\n",
        ")\n",
        "images_dict['128x128x128']['t2'] = images_dict['128x128x128']['t2'].reshape(\n",
        "    [images_dict['128x128x128']['t2'].shape[0], -1]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qty_A4eZ7Zyh",
        "cellView": "both"
      },
      "source": [
        "#@title The orginal T1 images have a total of 9175040 voxels. \n",
        "from IPython.display import Markdown as md\n",
        "md(\"The sizes for the  32x32x32 and 128x128x128 images are \\\n",
        "{} and {}, respectively. They represent {}% and \\\n",
        "{}% of the original size.\".format(images_dict['32x32x32']['t1'].shape[1], \n",
        "                                  images_dict['128x128x128']['t1'].shape[1],\n",
        "                                  np.round(images_dict['32x32x32']['t1'].shape[1] / 9175040 * 100, 2),\n",
        "                                  np.round(images_dict['128x128x128']['t1'].shape[1] / 9175040 * 100, 2),\n",
        "                                  )\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH0TXHyc7Zyp"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "pca32 = PCA(n_components=2)\n",
        "pca32.fit(\n",
        "    np.concatenate([\n",
        "        images_dict['32x32x32']['t1'][:30],\n",
        "        images_dict['32x32x32']['t2'][:30]\n",
        "    ], 0)\n",
        ")\n",
        "\n",
        "# Some samples (usually ~10-20%) are used as validation data that will not \n",
        "# be used to train the model.\n",
        "t1_transform_train = pca32.transform(images_dict['32x32x32']['t1'][:30])\n",
        "t2_transform_train = pca32.transform(images_dict['32x32x32']['t2'][:30])\n",
        "t1_transform_valid = pca32.transform(images_dict['32x32x32']['t1'][30:])\n",
        "t2_transform_valid = pca32.transform(images_dict['32x32x32']['t2'][30:])\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "\n",
        "blues = ['b' for _ in range(len(images_dict['32x32x32']['t1'][:30]))]\n",
        "greens = ['g' for _ in range(len(images_dict['32x32x32']['t2'][:30]))]\n",
        "reds = ['r' for _ in range(len(images_dict['32x32x32']['t1'][30:]))]\n",
        "cyans = ['c' for _ in range(len(images_dict['32x32x32']['t2'][30:]))]\n",
        "\n",
        "blue_patch = mpatches.Patch(color='b', label='T1 (train)')\n",
        "green_patch = mpatches.Patch(color='g', label='T2 (train)')\n",
        "red_patch = mpatches.Patch(color='r', label='T1 (valid)')\n",
        "cyan_patch = mpatches.Patch(color='c', label='T2 (valid)')\n",
        "\n",
        "plt.scatter(t1_transform_train[:, 0], t1_transform_train[:, 1], c=blues)\n",
        "plt.scatter(t2_transform_train[:, 0], t2_transform_train[:, 1], c=greens)\n",
        "plt.scatter(t1_transform_valid[:, 0], t1_transform_valid[:, 1], c=reds)\n",
        "plt.scatter(t2_transform_valid[:, 0], t2_transform_valid[:, 1], c=cyans)\n",
        "\n",
        "plt.title('PCA of images resized to 32x32x32')\n",
        "plt.legend()\n",
        "plt.xlabel('Component 1')\n",
        "plt.ylabel('Component 2')\n",
        "plt.legend(handles=[blue_patch, green_patch, red_patch, cyan_patch])\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "\n",
        "pca128 = PCA(n_components=2)\n",
        "pca128.fit(\n",
        "    np.concatenate([\n",
        "        images_dict['128x128x128']['t1'][:30],\n",
        "        images_dict['128x128x128']['t2'][:30]\n",
        "    ], 0)\n",
        ")\n",
        "\n",
        "t1_transform_train = pca128.transform(images_dict['128x128x128']['t1'][:30])\n",
        "t2_transform_train = pca128.transform(images_dict['128x128x128']['t2'][:30])\n",
        "t1_transform_valid = pca128.transform(images_dict['128x128x128']['t1'][30:])\n",
        "t2_transform_valid = pca128.transform(images_dict['128x128x128']['t2'][30:])\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "\n",
        "plt.scatter(t1_transform_train[:, 0], t1_transform_train[:, 1], c=blues)\n",
        "plt.scatter(t2_transform_train[:, 0], t2_transform_train[:, 1], c=greens)\n",
        "plt.scatter(t1_transform_valid[:, 0], t1_transform_valid[:, 1], c=reds)\n",
        "plt.scatter(t2_transform_valid[:, 0], t2_transform_valid[:, 1], c=cyans)\n",
        "\n",
        "plt.title('PCA of images resized to 128x128x128')\n",
        "plt.xlabel('Component 1')\n",
        "plt.ylabel('Component 2')\n",
        "\n",
        "plt.legend(handles=[blue_patch, green_patch, red_patch, cyan_patch])\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asMSP4Eo7Zys"
      },
      "source": [
        "#@title The orginal T1 images have a total of 9175040 voxels. \n",
        "from IPython.display import Markdown as md\n",
        "md(\"For the 128x128x128 voxel images, the first component of the PCA \"\n",
        "    \"explains ~{}% of the variance of the images and the second ~{}%. \"\n",
        "    \"For the 32x32x32 images, the first component explains {}% of the \"\n",
        "    \"variance and the second {}%\".format(\n",
        "        np.round(pca128.explained_variance_ratio_[0] * 100, 2),\n",
        "        np.round(pca128.explained_variance_ratio_[1] * 100, 2),\n",
        "        np.round(pca32.explained_variance_ratio_[0] * 100, 2),\n",
        "        np.round(pca32.explained_variance_ratio_[1] * 100, 2),\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jx_d7in7Zy5"
      },
      "source": [
        "## Basic machine learning classification model\n",
        "\n",
        "The classification in this tutorial is trivial, so a simple linear model like a logistic regression classifier should be able to learn hot to perfectly classify the images for both image sizes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vbGRrYhws7z"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "print('32x32x32')\n",
        "\n",
        "lr32 = LogisticRegression()\n",
        "labels = [0 for x in range(len(images_dict['32x32x32']['t1'][:30]))] + \\\n",
        "         [1 for x in range(len(images_dict['32x32x32']['t2'][:30]))]\n",
        "labels_valid = [0 for x in range(len(images_dict['32x32x32']['t1'][30:]))] + \\\n",
        "               [1 for x in range(len(images_dict['32x32x32']['t2'][30:]))]\n",
        "lr32.fit(\n",
        "    np.concatenate([\n",
        "        images_dict['32x32x32']['t1'][:30],\n",
        "        images_dict['32x32x32']['t2'][:30]\n",
        "    ], 0),\n",
        "    labels\n",
        ")\n",
        "\n",
        "# Labels T1 are 0s and T2 are 1\n",
        "labels_t1_train = [0 for _ in preds_t1]\n",
        "labels_t1_valid = [0 for _ in preds_t1_valid]\n",
        "labels_t2_train = [1 for _ in preds_t2]\n",
        "labels_t2_valid = [1 for _ in preds_t2_valid]\n",
        "\n",
        "preds_t1 = lr32.predict(images_dict['32x32x32']['t1'][:30])\n",
        "preds_t2 = lr32.predict(images_dict['32x32x32']['t2'][:30])\n",
        "preds_t1_valid = lr32.predict(images_dict['32x32x32']['t1'][30:])\n",
        "preds_t2_valid = lr32.predict(images_dict['32x32x32']['t2'][30:])\n",
        "\n",
        "accuracy = sum([1 if pred == target else 0 for (pred, target) in zip(\n",
        "    np.concatenate((preds_t1_train, preds_t2_train)),\n",
        "    np.concatenate((labels_t1_train, labels_t2_train)))]\n",
        ") / len(labels)\n",
        "\n",
        "accuracy_valid = sum([1 if pred == target else 0 for (pred, target) in zip(\n",
        "    np.concatenate((preds_t1_valid, preds_t2_valid)),\n",
        "    np.concatenate((labels_t1_valid, labels_t2_valid)))]\n",
        ") / len(labels_valid)\n",
        "\n",
        "print('Train Accuracy: ', accuracy)\n",
        "print('Valid Accuracy: ', accuracy_valid)\n",
        "\n",
        "print('128x128x128')\n",
        "\n",
        "lr128 = LogisticRegression()\n",
        "labels = [0 for x in range(len(images_dict['128x128x128']['t1'][:30]))] + \\\n",
        "         [1 for x in range(len(images_dict['128x128x128']['t2'][:30]))]\n",
        "labels_valid = [0 for x in range(len(images_dict['128x128x128']['t1'][30:]))] + \\\n",
        "               [1 for x in range(len(images_dict['32x32x32']['t2'][30:]))]\n",
        "\n",
        "lr128.fit(\n",
        "    np.concatenate([\n",
        "        images_dict['128x128x128']['t1'][:30],\n",
        "        images_dict['128x128x128']['t2'][:30]\n",
        "    ], 0),\n",
        "    labels\n",
        ")\n",
        "\n",
        "\n",
        "preds_t1_train = lr128.predict(images_dict['128x128x128']['t1'][:30])\n",
        "preds_t2_train = lr128.predict(images_dict['128x128x128']['t2'][:30])\n",
        "preds_t1_valid = lr128.predict(images_dict['128x128x128']['t1'][30:])\n",
        "preds_t2_valid = lr128.predict(images_dict['128x128x128']['t2'][30:])\n",
        "accuracy = sum([1 if pred == target else 0 for (pred, target) in zip(\n",
        "    np.concatenate((preds_t1_train, preds_t2_train)),\n",
        "     np.concatenate((labels_t1_train, labels_t2_train)))]\n",
        ") / len(labels)\n",
        "accuracy_valid = sum([1 if pred == target else 0 for (pred, target) in zip(\n",
        "    np.concatenate((preds_t1_valid, preds_t2_valid)), \n",
        "    np.concatenate((labels_t1_valid, labels_t2_valid)))]\n",
        ") / len(labels_valid)\n",
        "\n",
        "print('Train Accuracy: ', accuracy)\n",
        "print('Valid Accuracy: ', accuracy_valid)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
